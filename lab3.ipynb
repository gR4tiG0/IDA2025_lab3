{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "\n",
    "import zipfile\n",
    "import re\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Fully Connected Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: (284807, 31)\n",
      "Fraud: 492 (0.17%)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('creditcard.csv')\n",
    "print(f'Dataset: {df.shape}')\n",
    "print(f'Fraud: {df[\"Class\"].sum()} ({df[\"Class\"].mean()*100:.2f}%)')\n",
    "\n",
    "X = df.drop('Class', axis=1).values\n",
    "y = df['Class'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train).unsqueeze(1)),\n",
    "    batch_size=256, shuffle=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    TensorDataset(torch.FloatTensor(X_test), torch.FloatTensor(y_test).unsqueeze(1)),\n",
    "    batch_size=256, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(FCN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = FCN(X_train.shape[1]).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.0737\n",
      "Epoch 2/5, Loss: 0.0054\n",
      "Epoch 3/5, Loss: 0.0037\n",
      "Epoch 4/5, Loss: 0.0033\n",
      "Epoch 5/5, Loss: 0.0032\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch)\n",
    "        loss = criterion(output, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{epochs}, Loss: {train_loss/len(train_loader):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.999263\n",
      "F1-Score: 0.7805\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal       1.00      1.00      1.00     85295\n",
      "       Fraud       0.81      0.76      0.78       148\n",
      "\n",
      "    accuracy                           1.00     85443\n",
      "   macro avg       0.90      0.88      0.89     85443\n",
      "weighted avg       1.00      1.00      1.00     85443\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "y_pred, y_true = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        output = model(X_batch)\n",
    "        pred = (torch.sigmoid(output) > 0.5).float()\n",
    "        y_pred.extend(pred.cpu().numpy())\n",
    "        y_true.extend(y_batch.numpy())\n",
    "\n",
    "y_pred = np.array(y_pred).flatten()\n",
    "y_true = np.array(y_true).flatten()\n",
    "\n",
    "print(f'\\nAccuracy: {accuracy_score(y_true, y_pred):.6f}')\n",
    "print(f'F1-Score: {f1_score(y_true, y_pred):.4f}')\n",
    "print(classification_report(y_true, y_pred, target_names=['Normal', 'Fraud']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Model  Accuracy\n",
      "0       Random Forest  0.999508\n",
      "1                 kNN  0.999368\n",
      "2       Decision Tree  0.999368\n",
      "3                 SVM  0.999345\n",
      "5  Fully Connected NN  0.999263\n",
      "4            AdaBoost  0.999099\n"
     ]
    }
   ],
   "source": [
    "lab1_results = {\n",
    "    'Random Forest': 0.999508,\n",
    "    'kNN': 0.999368,\n",
    "    'Decision Tree': 0.999368,\n",
    "    'SVM': 0.999345,\n",
    "    'AdaBoost': 0.999099,\n",
    "    'Fully Connected NN': accuracy_score(y_true, y_pred)\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(list(lab1_results.items()), columns=['Model', 'Accuracy'])\n",
    "results_df = results_df.sort_values('Accuracy', ascending=False)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 10000, Test: 26032\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "train_full = datasets.SVHN(root='./data', split='train', download=True, transform=transform)\n",
    "test_dataset = datasets.SVHN(root='./data', split='test', download=True, transform=transform)\n",
    "\n",
    "train_dataset, _ = random_split(train_full, [10000, len(train_full) - 10000])\n",
    "\n",
    "train_loader_img = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader_img = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "print(f'Train: {len(train_dataset)}, Test: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2a: Simple CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 256)\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(torch.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(torch.relu(self.bn3(self.conv3(x))))\n",
    "        x = x.view(-1, 128 * 4 * 4)\n",
    "        x = self.dropout(torch.relu(self.fc1(x)))\n",
    "        return self.fc2(x)\n",
    "\n",
    "cnn = SimpleCNN().to(device)\n",
    "criterion_cnn = nn.CrossEntropyLoss()\n",
    "optimizer_cnn = optim.Adam(cnn.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss: 2.1991, Val Acc: 35.86%\n",
      "Epoch 2/3, Loss: 1.2410, Val Acc: 77.17%\n",
      "Epoch 3/3, Loss: 0.7979, Val Acc: 80.45%\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    cnn.train()\n",
    "    train_loss = 0\n",
    "    for imgs, labels in train_loader_img:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        optimizer_cnn.zero_grad()\n",
    "        output = cnn(imgs)\n",
    "        loss = criterion_cnn(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer_cnn.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    cnn.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in test_loader_img:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            output = cnn(imgs)\n",
    "            _, pred = torch.max(output, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (pred == labels).sum().item()\n",
    "    \n",
    "    acc = 100 * correct / total\n",
    "    print(f'Epoch {epoch+1}/{epochs}, Loss: {train_loss/len(train_loader_img):.4f}, Val Acc: {acc:.2f}%')\n",
    "\n",
    "simple_cnn_acc = acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2b: Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = models.resnet18(pretrained=True)\n",
    "\n",
    "for param in resnet.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "resnet.fc = nn.Linear(resnet.fc.in_features, 10)\n",
    "resnet = resnet.to(device)\n",
    "\n",
    "optimizer_resnet = optim.Adam(resnet.fc.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss: 2.2451, Val Acc: 25.01%\n",
      "Epoch 2/3, Loss: 2.0305, Val Acc: 28.29%\n",
      "Epoch 3/3, Loss: 1.9621, Val Acc: 29.06%\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    resnet.train()\n",
    "    train_loss = 0\n",
    "    for imgs, labels in train_loader_img:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        optimizer_resnet.zero_grad()\n",
    "        output = resnet(imgs)\n",
    "        loss = criterion_cnn(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer_resnet.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    resnet.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in test_loader_img:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            output = resnet(imgs)\n",
    "            _, pred = torch.max(output, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (pred == labels).sum().item()\n",
    "    \n",
    "    acc = 100 * correct / total\n",
    "    print(f'Epoch {epoch+1}/{epochs}, Loss: {train_loss/len(train_loader_img):.4f}, Val Acc: {acc:.2f}%')\n",
    "\n",
    "resnet_acc = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Simple CNN: 80.45%\n",
      "ResNet18 (pretrained): 29.06%\n",
      "Difference: -51.40%\n"
     ]
    }
   ],
   "source": [
    "print(f'\\nSimple CNN: {simple_cnn_acc:.2f}%')\n",
    "print(f'ResNet18 (pretrained): {resnet_acc:.2f}%')\n",
    "print(f'Difference: {resnet_acc - simple_cnn_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Messages: 5574, Spam: 747 (13.40%)\n"
     ]
    }
   ],
   "source": [
    "with zipfile.ZipFile('sms+spam+collection.zip', 'r') as z:\n",
    "    z.extractall('sms_data')\n",
    "\n",
    "texts, labels = [], []\n",
    "with open('sms_data/SMSSpamCollection', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        label, text = line.strip().split('\\t', 1)\n",
    "        labels.append(1 if label == 'spam' else 0)\n",
    "        texts.append(text.lower())\n",
    "\n",
    "print(f'Messages: {len(texts)}, Spam: {sum(labels)} ({sum(labels)/len(labels)*100:.2f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 5000\n"
     ]
    }
   ],
   "source": [
    "def preprocess(text):\n",
    "    return ' '.join(re.sub(r'[^a-z\\s]', '', text).split())\n",
    "\n",
    "texts = [preprocess(t) for t in texts]\n",
    "\n",
    "all_words = []\n",
    "for text in texts:\n",
    "    all_words.extend(text.split())\n",
    "\n",
    "word_counts = Counter(all_words)\n",
    "vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "for word, _ in word_counts.most_common(4998):\n",
    "    vocab[word] = len(vocab)\n",
    "\n",
    "print(f'Vocab size: {len(vocab)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_seq(text, max_len=100):\n",
    "    seq = [vocab.get(w, vocab['<UNK>']) for w in text.split()]\n",
    "    if len(seq) < max_len:\n",
    "        seq += [0] * (max_len - len(seq))\n",
    "    else:\n",
    "        seq = seq[:max_len]\n",
    "    return seq\n",
    "\n",
    "sequences = [to_seq(t) for t in texts]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    sequences, labels, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "train_loader_text = DataLoader(\n",
    "    TensorDataset(torch.LongTensor(X_train), torch.FloatTensor(y_train).unsqueeze(1)),\n",
    "    batch_size=64, shuffle=True\n",
    ")\n",
    "test_loader_text = DataLoader(\n",
    "    TensorDataset(torch.LongTensor(X_test), torch.FloatTensor(y_test).unsqueeze(1)),\n",
    "    batch_size=64, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected Positive Weight: 6.4565\n"
     ]
    }
   ],
   "source": [
    "num_pos = sum(y_train)\n",
    "num_neg = len(y_train) - num_pos\n",
    "\n",
    "# FIX: Invert the division. We want Negative / Positive\n",
    "# This will result in a weight > 1 (approx 6.4), forcing the model to pay attention to Spam\n",
    "pos_weight_val = num_neg / num_pos\n",
    "pos_weight = torch.tensor([pos_weight_val]).to(device)\n",
    "\n",
    "print(f\"Corrected Positive Weight: {pos_weight.item():.4f}\")\n",
    "\n",
    "# Re-define loss with correct weight\n",
    "criterion_lstm = nn.BCEWithLogitsLoss(pos_weight=pos_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3a: LSTM with Random Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=2, batch_first=True, dropout=0.3)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.dropout(self.embedding(x))\n",
    "        _, (h, _) = self.lstm(x)\n",
    "        return self.fc(self.dropout(h[-1]))\n",
    "\n",
    "lstm_random = LSTM(len(vocab), 128, 256).to(device)\n",
    "\n",
    "# Add class weights for imbalanced data\n",
    "pos_weight = torch.tensor([sum(y_train) / (len(y_train) - sum(y_train))]).to(device)\n",
    "criterion_lstm = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "optimizer_lstm = optim.Adam(lstm_random.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.1393, Val Acc: 86.64%\n",
      "Epoch 2/10, Loss: 0.0999, Val Acc: 86.64%\n",
      "Epoch 4/10, Loss: 0.0993, Val Acc: 86.64%\n",
      "Epoch 6/10, Loss: 0.0996, Val Acc: 86.64%\n",
      "Epoch 8/10, Loss: 0.0993, Val Acc: 86.64%\n",
      "Epoch 10/10, Loss: 0.0995, Val Acc: 86.64%\n"
     ]
    }
   ],
   "source": [
    "lstm_random = LSTM(len(vocab), 128, 256).to(device)\n",
    "\n",
    "optimizer_lstm = optim.Adam(lstm_random.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    lstm_random.train()\n",
    "    train_loss = 0\n",
    "    for seqs, lbls in train_loader_text:\n",
    "        seqs, lbls = seqs.to(device), lbls.to(device)\n",
    "        optimizer_lstm.zero_grad()\n",
    "        output = lstm_random(seqs)\n",
    "        loss = criterion_lstm(output, lbls)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(lstm_random.parameters(), 1.0)\n",
    "        optimizer_lstm.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    # Evaluation\n",
    "    lstm_random.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for seqs, lbls in test_loader_text:\n",
    "            seqs, lbls = seqs.to(device), lbls.to(device)\n",
    "            output = lstm_random(seqs)\n",
    "            pred = (torch.sigmoid(output) > 0.5)\n",
    "            correct += (pred == lbls).sum().item()\n",
    "            total += lbls.size(0)\n",
    "    \n",
    "    acc = 100 * correct / total\n",
    "    if epoch == 0 or (epoch + 1) % 2 == 0:\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {train_loss/len(train_loader_text):.4f}, Val Acc: {acc:.2f}%')\n",
    "\n",
    "lstm_random_acc = acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "at this point idk what to do, dataset is not balanced, so it just clasify everything as ham"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3b: LSTM with GloVe Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4076/5000 words in GloVe\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "\n",
    "glove_file = 'glove.6B.100d.txt'\n",
    "if not os.path.exists(glove_file):\n",
    "    print('Downloading GloVe...')\n",
    "    urllib.request.urlretrieve('http://nlp.stanford.edu/data/glove.6B.zip', 'glove.6B.zip')\n",
    "    print('Extracting GloVe...')\n",
    "    with zipfile.ZipFile('glove.6B.zip', 'r') as z:\n",
    "        z.extract(glove_file)\n",
    "    os.remove('glove.6B.zip')\n",
    "\n",
    "glove = {}\n",
    "with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        vals = line.split()\n",
    "        glove[vals[0]] = np.array(vals[1:], dtype='float32')\n",
    "\n",
    "embed_matrix = np.zeros((len(vocab), 100))\n",
    "found = 0\n",
    "for word, idx in vocab.items():\n",
    "    if word in glove:\n",
    "        embed_matrix[idx] = glove[word]\n",
    "        found += 1\n",
    "    else:\n",
    "        embed_matrix[idx] = np.random.normal(0, 0.1, 100)\n",
    "\n",
    "print(f'Found {found}/{len(vocab)} words in GloVe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMGloVe(nn.Module):\n",
    "    def __init__(self, embed_matrix, hidden_dim):\n",
    "        super(LSTMGloVe, self).__init__()\n",
    "        vocab_size, embed_dim = embed_matrix.shape\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.embedding.weight = nn.Parameter(torch.FloatTensor(embed_matrix))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=2, batch_first=True, dropout=0.3)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.dropout(self.embedding(x))\n",
    "        _, (h, _) = self.lstm(x)\n",
    "        return self.fc(self.dropout(h[-1]))\n",
    "\n",
    "lstm_glove = LSTMGloVe(embed_matrix, 256).to(device)\n",
    "optimizer_glove = optim.Adam(filter(lambda p: p.requires_grad, lstm_glove.parameters()), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.4170, Val Acc: 86.64%\n",
      "Epoch 2/5, Loss: 0.3940, Val Acc: 86.64%\n",
      "Epoch 3/5, Loss: 0.3973, Val Acc: 86.64%\n",
      "Epoch 4/5, Loss: 0.3981, Val Acc: 86.64%\n",
      "Epoch 5/5, Loss: 0.3959, Val Acc: 86.64%\n"
     ]
    }
   ],
   "source": [
    "lstm_glove = LSTMGloVe(embed_matrix, 256).to(device)\n",
    "\n",
    "# INCREASED LR to 0.001\n",
    "optimizer_glove = optim.Adam(filter(lambda p: p.requires_grad, lstm_glove.parameters()), lr=0.001)\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    lstm_glove.train()\n",
    "    train_loss = 0\n",
    "    for seqs, lbls in train_loader_text:\n",
    "        seqs, lbls = seqs.to(device), lbls.to(device)\n",
    "        optimizer_glove.zero_grad()\n",
    "        output = lstm_glove(seqs)\n",
    "        loss = criterion_lstm(output, lbls) # Uses the fixed weight from Step 1\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(lstm_glove.parameters(), 1.0)\n",
    "        optimizer_glove.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    lstm_glove.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for seqs, lbls in test_loader_text:\n",
    "            seqs, lbls = seqs.to(device), lbls.to(device)\n",
    "            output = lstm_glove(seqs)\n",
    "            pred = (torch.sigmoid(output) > 0.5)\n",
    "            correct += (pred == lbls).sum().item()\n",
    "            total += lbls.size(0)\n",
    "    \n",
    "    acc = 100 * correct / total\n",
    "    if epoch == 0 or (epoch + 1) % 2 == 0:\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {train_loss/len(train_loader_text):.4f}, Val Acc: {acc:.2f}%')\n",
    "\n",
    "lstm_glove_acc = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Embeddings: 86.64%\n",
      "GloVe Embeddings: 86.64%\n",
      "Improvement: 0.00%\n",
      "FCN (Fraud Detection): 0.999263\n",
      "Simple CNN (SVHN): 80.45%\n",
      "ResNet18 (SVHN): 29.06%\n",
      "LSTM Random (SMS): 86.64%\n",
      "LSTM GloVe (SMS): 86.64%\n"
     ]
    }
   ],
   "source": [
    "print(f'\\nRandom Embeddings: {lstm_random_acc:.2f}%')\n",
    "print(f'GloVe Embeddings: {lstm_glove_acc:.2f}%')\n",
    "print(f'Improvement: {lstm_glove_acc - lstm_random_acc:.2f}%')\n",
    "print(f'FCN (Fraud Detection): {accuracy_score(y_true, y_pred):.6f}')\n",
    "print(f'Simple CNN (SVHN): {simple_cnn_acc:.2f}%')\n",
    "print(f'ResNet18 (SVHN): {resnet_acc:.2f}%')\n",
    "print(f'LSTM Random (SMS): {lstm_random_acc:.2f}%')\n",
    "print(f'LSTM GloVe (SMS): {lstm_glove_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Performance Results\n",
    "\n",
    "| Model | Dataset | Accuracy |\n",
    "|-------|---------|----------|\n",
    "| Random Embeddings | SMS | 86.64% |\n",
    "| GloVe Embeddings | SMS | 86.64% |\n",
    "| **Improvement** | | **0.00%** |\n",
    "| FCN | Fraud Detection | 99.93% |\n",
    "| Simple CNN | SVHN | 80.45% |\n",
    "| ResNet18 | SVHN | 29.06% |\n",
    "| LSTM Random | SMS | 86.64% |\n",
    "| LSTM GloVe | SMS | 86.64% |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
